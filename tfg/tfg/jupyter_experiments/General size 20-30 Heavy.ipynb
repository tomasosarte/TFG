{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch as th\n",
    "from time import time\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from params import default_params\n",
    "\n",
    "# ------------------------- ENVIRONMENTS ---------------------------\n",
    "from environments.environment_tsp import EnviornmentTSP\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# ------------------------- NETWORKS -------------------------------\n",
    "from networks.attention_encoder_decoder import HeavyAttentionEncoderDecoder\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# ------------------------- EXPERIMENTS ----------------------------\n",
    "from experiments.actor_critic_experiment import ActorCriticExperiment\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# ------------------------- CONTROLLERS ----------------------\n",
    "from controllers.ac_controller import ActorCriticController\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# ------------------------- LEARNERS --------------------------------\n",
    "from learners.biased_reinforce_learner import BiasedReinforceLearner\n",
    "from learners.actor_critic_learner import ActorCriticLearner\n",
    "from learners.ppo_learner import PPOLearner\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# -------------------------- GENERATORS ----------------------------\n",
    "from generators.tsp_generator import TSPGenerator\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# -------------------------- SOLVERS ----------------------------\n",
    "from solvers.gurobi_tsp import solve_tsp\n",
    "# ------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device in use:  cpu\n",
      "Epsilon anneal time:  1320000.0\n",
      "Entropy anneal episodes:  3000.0\n",
      "Total transitions:  1650000\n"
     ]
    }
   ],
   "source": [
    "params = default_params()\n",
    "\n",
    "# Debugging outputs and plotting during training\n",
    "params['plot_frequency'] = 10\n",
    "params['plot_train_samples'] = True\n",
    "params['debug_messages'] = False\n",
    "params['print_dots'] = False\n",
    "\n",
    "# Environment parameters\n",
    "max_nodes_per_graph = 30\n",
    "params['env'] = 'tsp'\n",
    "params['node_dimension'] = 2\n",
    "params['max_nodes_per_graph'] = max_nodes_per_graph\n",
    "params['max_episode_length'] = max_nodes_per_graph + 1\n",
    "params['diff_cities'] = True\n",
    "params['use_training_set'] = False\n",
    "params['training_sizes'] = [20, 22, 24, 26, 28, 30]\n",
    "params['num_train_instance_per_size'] = 10\n",
    "params['cities'] = None\n",
    "\n",
    "# Runner parameters\n",
    "max_episodes = 3000\n",
    "rollouts_per_batch = 50\n",
    "params['max_episodes'] = max_episodes\n",
    "params['max_steps'] = params['max_episodes'] * params['max_episode_length'] * rollouts_per_batch\n",
    "params['multi_runner'] = False               \n",
    "params['parallel_environments'] = 2  \n",
    "\n",
    "# Exploration parameters\n",
    "pct_epsilon_anneal_time = 0.8\n",
    "params['use_epsilon_greedy'] = False\n",
    "params['epsilon_anneal_time'] =  pct_epsilon_anneal_time * params['max_steps']\n",
    "params['epsilon_finish'] = 1E-5\n",
    "params['epsilon_start'] = 1.0\n",
    "params['epsilon_decay'] = \"linear\"\n",
    "\n",
    "pct_entropy_anneal_time = 1.0\n",
    "params['entropy_regularization'] = True\n",
    "params['decay_entropy'] = True\n",
    "params['entropy_weight'] = 0.1\n",
    "params['entropy_weight_start'] = 1.0\n",
    "params['entropy_weight_end'] = 0.03\n",
    "params['entropy_anneal_time'] = pct_entropy_anneal_time * params['max_episodes']\n",
    "\n",
    "# Optimization parameters\n",
    "params['lr'] = 5E-4\n",
    "params['gamma'] = 0.99\n",
    "params['batch_size'] = params['max_episode_length'] * rollouts_per_batch\n",
    "params['grad_norm_clip'] = 1\n",
    "\n",
    "# Actor-critic parameters\n",
    "params['value_loss_param'] = 0.1\n",
    "params['advantage_bias'] = True\n",
    "params['advantage_bootstrap'] = True\n",
    "params['offpolicy_iterations'] = 10\n",
    "params['value_targets'] = 'td'\n",
    "\n",
    "# PPO parameters\n",
    "params['ppo_clipping'] = True\n",
    "params['ppo_clip_eps'] = 0.1\n",
    "\n",
    "# Network parameters\n",
    "params['embedding_dimension'] = 4                                    \n",
    "params['n_heads'] = 4               \n",
    "params['n_layers'] = 3 \n",
    "params['normalization'] = 'batch'\n",
    "params['feed_forward_hidden'] = 512      \n",
    "\n",
    "# Device\n",
    "# params['device'] = \"cuda\" if th.cuda.is_available() else \"cpu\"\n",
    "# th.device(params['device'])\n",
    "params['device'] = 'cpu'\n",
    "params['use_tqdm'] = False\n",
    "params['final_plot'] = False\n",
    "params['wandb'] = False\n",
    "\n",
    "print(\"Device in use: \", params['device'])\n",
    "print(\"Epsilon anneal time: \", params['epsilon_anneal_time'])\n",
    "print(\"Entropy anneal episodes: \", params['entropy_anneal_time'])\n",
    "print(\"Total transitions: \", params['max_steps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPOLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HeavyAttentionEncoderDecoder(params=params)\n",
    "env = EnviornmentTSP(params=params)\n",
    "\n",
    "# Run experiment\n",
    "experiment = ActorCriticExperiment(params, model, env, PPOLearner(model=model, params=params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sizes:   0%|                                                                                                                                                | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricted license - for non-production use only - expires 2025-11-24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sizes:   0%|                                                                                                                                                | 0/11 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected index [1, 64] to be smaller than self [21, 34] apart from dimension 0 and to be smaller size than src [1, 64]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_sizes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m21\u001b[39m,\u001b[38;5;241m22\u001b[39m,\u001b[38;5;241m23\u001b[39m,\u001b[38;5;241m24\u001b[39m,\u001b[38;5;241m25\u001b[39m,\u001b[38;5;241m26\u001b[39m,\u001b[38;5;241m27\u001b[39m,\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m29\u001b[39m,\u001b[38;5;241m30\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguroby_vs_greedy_rollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43msizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes_per_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage gap: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_gap\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/TFG/tfg/tfg/experiments/actor_critic_experiment.py:250\u001b[0m, in \u001b[0;36mActorCriticExperiment.guroby_vs_greedy_rollout\u001b[0;34m(self, sizes, num_episodes_per_size, plot)\u001b[0m\n\u001b[1;32m    247\u001b[0m optim_tour, optim_distance \u001b[38;5;241m=\u001b[39m solve_tsp(environment_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcities\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# Run the episode\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m greedy_tour \u001b[38;5;241m=\u001b[39m \u001b[43mnew_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuffer\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactions\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    251\u001b[0m greedy_distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_tour_distance(environment_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcities\u001b[39m\u001b[38;5;124m'\u001b[39m], greedy_tour, size)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# Calculate the gap\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/TFG/tfg/tfg/runners/runner.py:167\u001b[0m, in \u001b[0;36mRunner.run_episode\u001b[0;34m(self, transition_buffer, trim, return_dict)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_episode\u001b[39m(\u001b[38;5;28mself\u001b[39m, transition_buffer: TransitionBatch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, trim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    154\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m    Returns one episode in the enviornment.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    Returns a dictionary containing the transition_buffer, and the episode statistics.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m        dict: A dictionary containing the buffer, the episode reward, the episode length, and the number of environment steps.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransition_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/TFG/tfg/tfg/runners/runner.py:125\u001b[0m, in \u001b[0;36mRunner.run\u001b[0;34m(self, n_steps, transition_buffer, trim, return_dict)\u001b[0m\n\u001b[1;32m    123\u001b[0m state, reward, done, next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msum_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m--> 125\u001b[0m \u001b[43mmy_transition_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap_transition\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39melapsed_time \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepi_len: done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Compute discounted returns if episode is done or max_steps is reached\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/TFG/tfg/tfg/utils/transition_batch.py:167\u001b[0m, in \u001b[0;36mTransitionBatch.add\u001b[0;34m(self, trans)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m n \u001b[38;5;241m==\u001b[39m v\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall tensors in a transition need to have the same batch_size\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    166\u001b[0m     idx \u001b[38;5;241m=\u001b[39m idx\u001b[38;5;241m.\u001b[39mview(idx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m[\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(v\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)])\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Increase the size (and handle overflow)\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m n\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected index [1, 64] to be smaller than self [21, 34] apart from dimension 0 and to be smaller size than src [1, 64]"
     ]
    }
   ],
   "source": [
    "test_sizes = [20,21,22,23,24,25,26,27,28,29,30]\n",
    "metadata = experiment.guroby_vs_greedy_rollout(sizes=test_sizes, num_episodes_per_size=10)\n",
    "print(f\"Average gap: {metadata['avg_gap']}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = experiment.guroby_vs_best_sample(sizes=test_sizes, num_episodes_per_size=10, runs_per_episode=10)\n",
    "print(f\"Average gap: {metadata['avg_gap']}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "experiment.run()\n",
    "end = time()\n",
    "total_time = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total spended time in experiment: \", total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.plot_rollout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = experiment.guroby_vs_greedy_rollout(sizes=test_sizes, num_episodes_per_size=10)\n",
    "print(f\"Average gap: {metadata['avg_gap']}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = experiment.guroby_vs_best_sample(sizes=test_sizes, num_episodes_per_size=10, runs_per_episode=10)\n",
    "print(f\"Average gap: {metadata['avg_gap']}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
